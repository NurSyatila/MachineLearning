{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb316dd-c51d-419f-9f05-553dc0901bda",
   "metadata": {},
   "source": [
    "# 8.0 Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "This lesson will cover model evaluation techniques and hyperparameter tuning in machine learning. These two concepts are crucial for improving the performance of machine learning models and ensuring that they generalize well to unseen data. \n",
    "\n",
    "**Learning Objectives:**\n",
    "By the end of this lesson, students will be able to:\n",
    "\n",
    "* Understand the importance of model evaluation.\n",
    "* Evaluate models using different metrics depending on the problem type (regression or classification).\n",
    "* Implement model validation techniques like cross-validation.\n",
    "* Understand the role of hyperparameters in model training.\n",
    "* Use techniques such as grid search and random search to tune hyperparameters.\n",
    "\n",
    "## 8.1. Model Evaluation\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Training data:** Used to train the model.\n",
    "* **Testing data:** Used to evaluate model performance.\n",
    "* **Overfitting:** When a model performs well on the training data but poorly on unseen data because it has learned noise or random fluctuations.\n",
    "* **Underfitting:** When a model is too simple and fails to capture underlying patterns in the data.\n",
    "* **Generalization:** The model’s ability to perform well on new, unseen data.\n",
    "\n",
    "Evaluating model performance ensures it is not just memorizing data (overfitting) or ignoring patterns (underfitting). Performance should be assessed on data separate from the training set.\n",
    "\n",
    "### 8.1.1. Model Evaluation Metrics for Classification Problems\n",
    "* **Accuracy:** The percentage of correct predictions out of all predictions. In imbalanced datasets (where one class is much more frequent than the other), accuracy might be misleading.\n",
    "* **Precision:** The ratio of correctly predicted positive observations to all predicted positives. This metric is useful when the cost of false positives is high (e.g., predicting whether a customer will churn).\n",
    "* **Recall (sensitivity):** The ratio of correctly predicted positive observations to all actual positives. This is useful when the cost of false negatives is high (e.g., medical diagnoses where failing to detect a disease could be fatal).\n",
    "* **F1-Score:** The harmonic mean of precision and recall, providing a balance between the two. This metric is useful when you want a single metric that combines precision and recall.\n",
    "* **ROC Curve & AUC:** Used to evaluate binary classifiers, showing performance at various threshold settings.\n",
    "* **Confusion Matrix:** A table showing the true vs. predicted classifications. This allows you to calculate precision, recall, accuracy, and F1-score.\n",
    "\n",
    "**Hands-on Example: Evaluating a Classification Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8985daa2-c94b-49d8-8d78-ec5974bfabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a classifier\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52f584f-b430-4287-a43a-875973beeafc",
   "metadata": {},
   "source": [
    "### 8.1.2. Model Evaluation Metrics for Regression Problems\n",
    "* **Mean Absolute Error (MAE):** The average of the absolute differences between predicted and actual values. It is useful to calculate the absolute size of errors.\n",
    "* **Mean Squared Error (MSE):** The average of the squared differences between predicted and actual values. It is useful when you want to penalize larger errors more heavily.\n",
    "* **Root Mean Squared Error (RMSE):** The square root of MSE, which brings the error back to the original unit of measurement.\n",
    "* **R-squared (R²):** A measure of how well the model explains the variance in the target variable. It is useful to measure the proportion of the variance in the target variable that is explained by the model.\n",
    "  \n",
    "**Hands-on Example: Evaluating a Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538831db-23cb-40ef-98f2-b744e0c873bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load dataset\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a regression model\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(\"R^2:\", r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5ba31-f1c9-4375-b365-cb33afcd4d4e",
   "metadata": {},
   "source": [
    "### 8.1.3. Cross-Validation\n",
    "Cross-validation is a technique used to assess how well a model generalizes by splitting the data into multiple training and testing sets. It is especially useful when the dataset is small. It reduces the variance of performance estimates and helps mitigate overfitting.\n",
    "\n",
    "**K-Fold Cross-Validation**\n",
    "* The dataset is split into *k* equal-sized folds.\n",
    "* The model is trained *k* times, each time using *k−1* folds for training and the remaining fold for testing.\n",
    "* The average performance across all *k* folds is reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f70f2a2-34c1-42ee-abfb-30eca8217a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation on a classifier\n",
    "cross_val_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(\"Cross-validation scores:\", cross_val_scores)\n",
    "print(\"Average cross-validation score:\", cross_val_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d60c5-fc55-4dfb-9cd8-73bc265f2fbe",
   "metadata": {},
   "source": [
    "## 8.2. Hyperparameter Tuning\n",
    "Hyperparameters are parameters that are set before the learning process begins (e.g., learning rate, number of trees in a forest, etc.). Tuning them is essential for optimizing model performance. The right hyperparameters can significantly improve model performance, while poor choices can lead to overfitting, underfitting, or slow training times.\n",
    "\n",
    "### 8.2.1. Introduction to Hyperparameters\n",
    "\n",
    "A hyperparameter is a configuration variable in machine learning algorithms that is set before training the model and governs the training process or the structure of the model itself (e.g., learning rate, number of trees in a Random Forest, number of layers in a neural network). Unlike parameters, which are learned from the data during training (such as the weights in a neural network), hyperparameters control how the model learns or the model's architecture.\n",
    "\n",
    "**Examples of Hyperparameters:**\n",
    "1. **Model Architecture Hyperparameters:**\n",
    "* Number of layers: In a neural network, you can decide how many hidden layers the model should have.\n",
    "* Number of neurons in each layer: In a neural network, this controls how many neurons each layer contains.\n",
    "Kernel size (for Convolutional Neural Networks, CNNs): Determines the dimensions of the kernel in convolutional layers.\n",
    "* Tree depth (for Decision Trees, Random Forests): Controls the maximum depth of the tree, affecting its complexity.\n",
    "\n",
    "2. **Training Process Hyperparameters:**\n",
    "* Learning rate: Controls how quickly or slowly the model adjusts its parameters during training (gradient descent). A higher learning rate means the model might converge faster, but too high a rate might cause it to overshoot the optimal solution.\n",
    "* Batch size: The number of training samples used in one iteration of model training.\n",
    "* Epochs: The number of times the entire dataset is passed through the model during training.\n",
    "* Momentum: In gradient-based optimization methods, momentum helps smooth out the updates to the model weights.\n",
    "\n",
    "3. **Regularization Hyperparameters:**\n",
    "* L1 or L2 regularization: These regularizers help prevent overfitting by adding penalties to large weights (L2) or enforcing sparsity (L1).\n",
    "* Dropout rate: In neural networks, dropout randomly disables a fraction of neurons to prevent overfitting during training.\n",
    "\n",
    "4. **Optimization Hyperparameters:**\n",
    "* Optimization algorithm: Whether to use algorithms like Stochastic Gradient Descent (SGD), Adam, or RMSprop to update model parameters.\n",
    "* Beta values (for algorithms like Adam): These control the moving averages of the gradients and squared gradients during training.\n",
    "\n",
    "Other than that, some hyperparameters are specific to model architecture. For example, in Random Forest, some hyperparameters can be optimized, such as number of trees, max features (controls how many features to consider when splitting a node) and max depth (limits the depth of the trees in a decision tree or random forest to prevent overfitting).\n",
    "\n",
    "### 8.2.2. Methods of hyperparameter tuning\n",
    "\n",
    "* **Grid Search:** Exhaustively tests a range of hyperparameters, evaluates model performance for each combination, and selects the best one.\n",
    "* **Random Search:** Randomly samples combinations of hyperparameters, offering a more efficient search compared to grid search. It is faster than grid search and can explore a larger space in a shorter amount of time, but doesn’t guarantee finding the best combination.\n",
    "* **Bayesian Optimization:** A probabilistic model that uses past evaluations to guide the search for the optimal hyperparameters. It is more efficient than grid search and random search and often requires fewer evaluations to find the best parameters. In Python, one popular library for Bayesian Optimization is `Hyperopt` or `Optuna`.\n",
    "\n",
    "**Grid Search (Example with Random Forest):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db68948b-357d-4f47-a663-a8d1994ea4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create a RandomForest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21f1b6-07ba-4e61-96ba-8e4dde80fb7c",
   "metadata": {},
   "source": [
    "**Random Search (Example with Random Forest):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d204dcd7-193d-43f6-b5bf-ebbc31d5eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "# Create a RandomForest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameters distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 200),\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20)\n",
    "}\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters:\", random_search.best_params_)\n",
    "print(\"Best score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af739f7d-4b54-4503-9a55-410eee47943b",
   "metadata": {},
   "source": [
    "**Bayesian Optimization (Example with Hyperopt using RandomForestClassifier)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e032b3-1034-47bf-b9e4-13333ce3dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the objective function for hyperparameter tuning\n",
    "def objective(params):\n",
    "    rf = RandomForestClassifier(**params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return -accuracy  # Return negative accuracy for minimization\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [10, 50, 100, 200]),\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4])\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization with Tree-structured Parzen Estimator (TPE)\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20, trials=trials)\n",
    "\n",
    "# Print the best found hyperparameters\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d01ed-4511-4f3e-802f-ff4c51f35006",
   "metadata": {},
   "source": [
    "**Practical**\n",
    "* Step 1: Load a dataset (e.g., Iris dataset or Boston housing dataset).\n",
    "* Step 2: Split the data into training and testing sets. Use train_test_split from sklearn for this.\n",
    "* Step 3: Train a model (e.g., Logistic Regression or Random Forest).\n",
    "* Step 4: Evaluate the model using the appropriate metrics (accuracy, precision, recall, etc.). Use cross_val_score or GridSearchCV to apply cross-validation.\n",
    "* Step 5: Tune hyperparameters using GridSearchCV or RandomizedSearchCV: Choose a set of hyperparameters to tune (e.g., max_depth, n_estimators for a Random Forest).\n",
    "* Run the search and evaluate performance.\n",
    "* Step 6: Compare model performance before and after hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
