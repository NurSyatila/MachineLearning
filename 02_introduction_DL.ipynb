{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d69e71d3-0541-444b-aa69-87bf21b80243",
   "metadata": {},
   "source": [
    "# 2.0. Introduction to Deep Learning\n",
    "\n",
    "**Learning Objectives:** By the end of this lesson, you should be able to:\n",
    "\n",
    "* Understand what deep learning is and how it differs from traditional machine learning.\n",
    "* Understand the basic structure of a neural network.\n",
    "* Gain familiarity with key concepts like neurons, layers, activation functions, and loss functions.\n",
    "* Build and train a simple deep learning model using TensorFlow or PyTorch.\n",
    "* Evaluate a deep learning model on a sample dataset.\n",
    "  \n",
    "Deep Learning is a subset of machine learning, which itself is a subset of artificial intelligence. It uses neural networks with many layers (hence the term \"deep\") to learn from large amounts of data.\n",
    "\n",
    "**Neural Networks:** Neural networks are inspired by the human brain and consist of interconnected \"neurons\" (nodes). These networks learn patterns and relationships in data to make predictions or decisions.\n",
    "\n",
    "**Key difference from traditional ML:** In traditional machine learning, you manually extract features from data (e.g., using techniques like PCA or decision trees). In deep learning, the model automatically learns features directly from raw data (e.g., images, text).\n",
    "\n",
    "# 2.1. Key Concepts in Deep Learning\n",
    "\n",
    "* **Loss Function:** A function that measures how well the model's predictions match the true labels. The goal is to minimize the loss function during training. Common loss functions include cross-entropy loss that is often used for classification tasks, and mean squared error (MSE) that is often used for regression tasks.\n",
    "* **Optimization:** Gradient Descent is the most commonly used optimization algorithm. It iteratively adjusts the weights to minimize the loss function. Some variants of Gradient Descent:\n",
    "  - Batch Gradient Descent: Uses the entire dataset to compute the gradient.\n",
    "  - Stochastic Gradient Descent (SGD): Uses one data point at a time.\n",
    "  - Mini-Batch Gradient Descent: Uses a small subset of data.\n",
    "* **Overfitting and Regularization:** Overfitting occurs when the model learns the training data too well and loses the ability to generalize to new data. Regularization techniques like dropout, L2 regularization, and early stopping help prevent overfitting.\n",
    "                     \n",
    "# 2.2. Neural Networks\n",
    "## 2.2.1. Basic Structure of a Neural Network\n",
    "\n",
    "* **Neurons:** Basic units that take inputs, apply weights, pass through an activation function, and output a value.\n",
    "Layers:\n",
    "* **Input Layer:** The initial layer that receives the raw input data (e.g., pixels of an image, or features of a dataset).\n",
    "* **Hidden Layers:** Layers between the input and output layers where the model learns to extract features.\n",
    "* **Output Layer:** The final layer that produces the prediction or output of the model.\n",
    "\n",
    "## 2.2.2. Key Components of Neural Networks\n",
    "\n",
    "* **Weights:** These determine the strength of the connection between neurons.\n",
    "* **Biases:** Additional parameters that help the model make better predictions.\n",
    "* **Activation Function:** After the weighted sum of inputs is calculated, the activation function determines whether a neuron should \"fire\" or not. Common activation functions include: ReLU (Rectified Linear Unit), Sigmoid, and Tanh, amongst any others. \n",
    "* **Forward Propagation:** The process of passing the input data through the network layer by layer to make a prediction.\n",
    "* **Backpropagation:** A method for updating the weights of the network to minimize the error between the predicted and actual output. This is done using optimization algorithms like Gradient Descent.\n",
    "\n",
    "## 2.2.3. Types of Neural Networks\n",
    "DL includes various types of learning:\n",
    "\n",
    "* **Feedforward Neural Networks (FNN):**\n",
    "  - The most basic type of neural network, where information flows in one direction, from input to output.\n",
    "  - Used for simple tasks like classification or regression.\n",
    "* **Convolutional Neural Networks (CNNs):**\n",
    "  - Primarily used for image data (e.g., object detection, image classification).\n",
    "  - Uses convolutional layers that apply filters to input images to extract spatial hierarchies of features.\n",
    "  - Contains pooling layers to reduce dimensionality.\n",
    "* **Recurrent Neural Networks (RNNs):**\n",
    "  - Designed for sequential data like time series or text.\n",
    "  - RNNs have \"memory\" that allows them to retain information from previous time steps in the sequence.\n",
    "* **Generative Adversarial Networks (GANs):**\n",
    "  - A class of models that consist of two networks (a generator and a discriminator) that are trained together in a competitive setting.\n",
    "  - Used for tasks like image generation, style transfer, and data augmentation.\n",
    "\n",
    "**Applications of deep learning:**\n",
    "* Image recognition (e.g., identifying objects in photos)\n",
    "* Game playing (e.g., AlphaGo).                                              \n",
    "* Speech recognition (e.g., Siri, Alexa)\n",
    "* Natural Language Processing (e.g., BERT, GPT for text generation and translation)\n",
    "* Autonomous vehicles (e.g., self-driving cars using vision and sensor data)\n",
    "\n",
    "# 2.3. Steps in Building a Deep Learning Model\n",
    "1. **Define the Problem**\n",
    "* Objective: Identify the problem you are trying to solve (e.g., classification, regression, image recognition, etc.).\n",
    "* Input/Output: Understand what the input data is (e.g., images, text, time-series) and what the desired output is (e.g., class labels, continuous values, sequences).\n",
    "* Evaluation Metric: Choose an appropriate metric to evaluate model performance (e.g., accuracy, F1-score, mean squared error, etc.).\n",
    "* Example: For image classification, the goal is to assign each image to a class label, and the metric could be accuracy.\n",
    "\n",
    "2. **Prepare the Data**\n",
    "* Collect Data: Gather the relevant dataset for your task. For supervised learning, you need labeled data.\n",
    "* Clean Data: Handle missing values, remove duplicates, and address any data quality issues.\n",
    "* Preprocess Data:\n",
    "  - Scaling/Normalization: Scale features to a similar range (e.g., normalizing pixel values between 0 and 1 for image data).\n",
    "  - Encoding: Convert categorical variables to numerical format (e.g., one-hot encoding for categorical labels).\n",
    "  - Reshaping: For image data, ensure that the images are reshaped into a consistent size (e.g., 28x28 pixels for MNIST).\n",
    "* Split Data: Split the dataset into training, validation, and test sets (typically 70%-80% for training, 10%-15% for validation, and 10%-15% for testing).\n",
    "* Example: For image data, resize the images to a fixed size (e.g., 28x28 pixels) and normalize the pixel values to a range of 0-1.\n",
    "\n",
    "3. **Build the Neural Network Architecture**\n",
    "* Define Layers: Specify the number of layers and the type of layers to use in the model:\n",
    "  - Input Layer: The first layer that takes in the input data.\n",
    "  - Hidden Layers: Layers in between the input and output layers where the learning happens. You can experiment with different architectures (e.g., shallow vs. deep networks).\n",
    "  - Output Layer: The last layer that produces the output (e.g., a single neuron for regression, or multiple neurons with softmax for classification).\n",
    "  - Activation Functions: Choose appropriate activation functions for each layer (e.g., ReLU for hidden layers, softmax for the output layer in classification).\n",
    "* Choice of Model Type:\n",
    "  - Feedforward Neural Networks (FNN) for basic tasks.\n",
    "  - Convolutional Neural Networks (CNNs) for image data.\n",
    "  - Recurrent Neural Networks (RNNs) for sequential data (e.g., text, time-series).\n",
    "\n",
    "4. **Choose a Loss Function**\n",
    "* The loss function measures how well the model's predictions match the actual results. The goal during training is to minimize the loss.\n",
    "\n",
    "* Common Loss Functions:\n",
    "  - Binary Cross-Entropy: Used for binary classification.\n",
    "  - Categorical Cross-Entropy: Used for multi-class classification.\n",
    "  - Mean Squared Error (MSE): Used for regression tasks.\n",
    "* Example: If you're doing a binary classification task, you might use binary cross-entropy as the loss function.\n",
    "\n",
    "5. **Choose an Optimizer**\n",
    "* The optimizer adjusts the weights of the network based on the gradients of the loss function during backpropagation.\n",
    "* Popular Optimizers:\n",
    "  - Stochastic Gradient Descent (SGD): Simple and widely used.\n",
    "  - Adam: Adaptive moment estimation (a variant of SGD) that often works well out of the box for deep learning models.\n",
    "  - RMSprop: Another variant of SGD that adapts the learning rate during training.\n",
    "* Example: Adam is often a good choice for training neural networks.\n",
    "\n",
    "6. **Compile the Model**\n",
    "* Compile the model by specifying the loss function, optimizer, and evaluation metrics. This step prepares the model for training.\n",
    "```model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])```\n",
    "\n",
    "7. **Train the Model**\n",
    "* Feed the training data: Train the model on the training set using the fit() method.\n",
    "* Validation Data: Use validation data to monitor the performance of the model on unseen data during training.\n",
    "* Epochs: The number of times the entire dataset is passed through the network.\n",
    "* Batch Size: The number of samples processed before the model's internal parameters are updated.\n",
    "```model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))```\n",
    "\n",
    "During training, the model adjusts its weights to minimize the loss function, and validation accuracy helps track its generalization performance.\n",
    "\n",
    "8. **Evaluate the Model**\n",
    "* Once training is complete, evaluate the model on the test data to see how well it generalizes to unseen data.\n",
    "* Metrics: Use appropriate metrics (e.g., accuracy, precision, recall, F1-score) to assess the performance.\n",
    "```test_loss, test_acc = model.evaluate(X_test, y_test)```\n",
    "```print(f'Test accuracy: {test_acc:.4f}')```\n",
    "\n",
    "9. **Tuning Hyperparameters**\n",
    "* Hyperparameter Tuning: Experiment with different configurations such as the number of layers, number of neurons in each layer, learning rate, batch size, and optimizer choice.\n",
    "* Grid Search or Random Search can be used to systematically search over a range of hyperparameters.\n",
    "* Use cross-validation (e.g., k-fold cross-validation) to ensure that the model performs well on different subsets of the data.\n",
    "\n",
    "10. **Save and Deploy the Model**\n",
    "* Save the trained model for future use, so you don’t have to retrain it every time.\n",
    "* Export the model (e.g., in .h5 format in Keras) and deploy it in production or integrate it into an application.\n",
    "```model.save('model.h5')  # Save the model```\n",
    "* Deployment: The model can be deployed via cloud services (e.g., AWS, Google Cloud) or integrated into a web application, mobile app, or IoT device.\n",
    "\n",
    "**Example Workflow in TensorFlow/Keras (Classification task):**                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eed0615-f83d-496d-8346-7c869e8ffc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 577us/step - accuracy: 0.8666 - loss: 0.4789 - val_accuracy: 0.9559 - val_loss: 0.1549\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 534us/step - accuracy: 0.9591 - loss: 0.1394 - val_accuracy: 0.9628 - val_loss: 0.1257\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544us/step - accuracy: 0.9747 - loss: 0.0888 - val_accuracy: 0.9695 - val_loss: 0.0982\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 537us/step - accuracy: 0.9815 - loss: 0.0621 - val_accuracy: 0.9718 - val_loss: 0.0956\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 561us/step - accuracy: 0.9857 - loss: 0.0481 - val_accuracy: 0.9730 - val_loss: 0.0863\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 220us/step - accuracy: 0.9702 - loss: 0.0911\n",
      "Test accuracy: 0.9747999906539917\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(60000, 28 * 28) / 255\n",
    "X_test = X_test.reshape(10000, 28 * 28) / 255\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Dense(128, input_dim=28*28, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88262294-01a7-493e-966b-714ee290d4a3",
   "metadata": {},
   "source": [
    "**Example Workflow in TensorFlow/Keras (Regression task):** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a7f90f-84fd-4f35-90bd-a9644c6b5816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 1: Load the data\n",
    "# Assuming you have a CSV file where 'target' is the column to predict\n",
    "data = pd.read_csv(\"data.csv\")  # Replace with your actual dataset\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['target'])  # Features (all columns except 'target')\n",
    "y = data['target']  # Target variable\n",
    "\n",
    "# Step 2: Data Preprocessing\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Build the Model\n",
    "# Create a simple feedforward neural network for regression\n",
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(X_train.shape[1],)),  # Input layer\n",
    "    keras.layers.Dense(64, activation='relu'),  # First hidden layer with 64 neurons and ReLU activation\n",
    "    keras.layers.Dense(32, activation='relu'),  # Second hidden layer with 32 neurons and ReLU activation\n",
    "    keras.layers.Dense(1)  # Output layer for regression (1 output for continuous prediction)\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer='adam',  # Optimizer (Adam is a good default choice)\n",
    "              loss='mean_squared_error',  # Loss function (MSE for regression)\n",
    "              metrics=['mae'])  # We can also track Mean Absolute Error (MAE)\n",
    "\n",
    "# Step 5: Train the Model\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50,  # Number of epochs\n",
    "                    batch_size=32,  # Batch size\n",
    "                    validation_data=(X_test, y_test),  # Validation data to track performance\n",
    "                    verbose=1)  # Verbosity level (1: shows progress bar)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Test Loss (MSE): {test_loss}\")\n",
    "print(f\"Test MAE (Mean Absolute Error): {test_mae}\")\n",
    "\n",
    "# Step 7: Make Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 8: Calculate the Mean Squared Error (MSE) manually\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
    "\n",
    "# Optional: Save the trained model\n",
    "model.save(\"regression_model.h5\")  # Save the model in HDF5 format\n",
    "\n",
    "# Optional: Load the saved model and make predictions\n",
    "loaded_model = keras.models.load_model(\"regression_model.h5\")\n",
    "y_loaded_pred = loaded_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab18e7c7-70a0-4bab-9f15-26ffabdbc7b3",
   "metadata": {},
   "source": [
    "**Model Performance Visualizations:**\n",
    "You can also plot the training and validation loss/MAE curves to visualize model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7e8d1-0dc2-4f5a-a47c-fcebee1dc0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation MAE values\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7c9e4-a756-402e-a861-f16685c92f8f",
   "metadata": {},
   "source": [
    "# 2.4. Example: Building a Simple Deep Learning Model\n",
    "**Step-by-Step Code Walkthrough:**\n",
    "\n",
    "**1. Import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc191b-0d13-47c4-89ce-60069c52b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211a182-c7ea-4b1f-8516-e384e2bc9971",
   "metadata": {},
   "source": [
    "**2. Load the Dataset (Using MNIST, a dataset of handwritten digits)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533c1ffc-f7b7-498d-9e0b-f9602c6f3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "X_train = X_train.reshape(60000, 28 * 28)  # Flatten the images\n",
    "X_test = X_test.reshape(10000, 28 * 28)\n",
    "X_train = X_train.astype('float32') / 255  # Normalize the data\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d83ec-ff64-472b-9302-e7e511ae7f57",
   "metadata": {},
   "source": [
    "**3. Build the Neural Network Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4c4e2-bc91-4e16-9a38-630c569f91ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple feedforward neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=28*28, activation='relu'))  # First hidden layer\n",
    "model.add(Dense(10, activation='softmax'))  # Output layer for 10 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9961f-784d-423e-8a00-b5eb08ebc897",
   "metadata": {},
   "source": [
    "**4. Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87139cc-5b2a-49d9-85a1-7ac49ceb6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the training data\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46040a75-bec7-447f-919b-8550aeb1dc66",
   "metadata": {},
   "source": [
    "**5. Evaluate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da688b4-dce5-4910-8f6f-650771295754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fd9bc-fa9b-4807-acba-f502a3cd2725",
   "metadata": {},
   "source": [
    "**Homework:** Experiment with different neural network architectures (e.g., adding more hidden layers, changing activation functions) and see how the performance changes.\n",
    "\n",
    "**Resources:**\n",
    "* TensorFlow Documentation\n",
    "* PyTorch Documentation\n",
    "* Deep Learning with Python by François Chollet (Book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ce10ef-12be-4e3a-8686-d4dd8cd0f680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
