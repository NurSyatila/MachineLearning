{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5552f8d-aaab-4d10-ba9e-23cc46e3b8ee",
   "metadata": {},
   "source": [
    "# 7.0 Supervised learning algorithms\n",
    "This lesson provides an overview of several key unsupervised learning algorithms and their practical implementation.\n",
    "\n",
    "**Lesson Objectives:** By the end of the lesson, students should be able to:\n",
    "* Understand the basic concepts of unsupervised learning algorithms.\n",
    "* Implement key unsupervised learning algorithms using real-world datasets.\n",
    "\n",
    "Unsupervised learning algorithms are used when the data does not have labeled output or target variables. The goal is to infer the underlying structure or distribution in the data. Unlike supervised learning, unsupervised learning doesn't have a predefined target variable, so the focus is on discovering patterns, groupings, or relationships within the data.\n",
    "\n",
    "# 7.1. Clustering\n",
    "Clustering algorithms group similar data points together based on their features. This is useful for discovering inherent structures in the data.\n",
    "\n",
    "## 7.1.1. K-Means Clustering\n",
    "K-Means is one of the most popular clustering algorithms. It partitions data into *k* distinct, non-overlapping clusters based on feature similarity.\n",
    "\n",
    "**How it works:**\n",
    "1. Choose k initial centroids (randomly or using heuristics).\n",
    "2. Assign each data point to the closest centroid.\n",
    "3. Recompute the centroids based on the points assigned to them.\n",
    "4. Repeat steps 2 and 3 until convergence (centroids don't change significantly).\n",
    "\n",
    "**Key Concept:** The algorithm minimizes the within-cluster sum of squares (WCSS), also known as inertia, to achieve compact and well-separated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64c3d1c-7d06-4e26-b268-b83f866e085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Apply KMeans with 4 clusters\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get cluster centers and labels\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Plot the data and cluster centers\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='X', s=200, c='red')\n",
    "plt.title(\"K-Means Clustering\")\n",
    "plt.show()\n",
    "# In this example, K-Means finds 4 clusters in the data, \n",
    "# and the red 'X' marks the centroids of each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e40563d-dc1d-41f2-8630-13b5a397524f",
   "metadata": {},
   "source": [
    "## 7.1.2. Hierarchical Clustering\n",
    "Hierarchical clustering builds a tree of clusters (also called a dendrogram). It can be agglomerative (bottom-up) or divisive (top-down).\n",
    "\n",
    "**How it works:**\n",
    "* **Agglomerative:** Starts with each point as its own cluster, and iteratively merges the closest pairs of clusters.\n",
    "* **Divisive:** Starts with all points in a single cluster, and recursively splits it into smaller clusters.\n",
    "\n",
    "**Key Concept:** The result is a tree-like structure that represents different levels of similarity. The user can choose a level of the tree to \"cut\" to get a specific number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af08f1-f1e1-4ded-9ce9-c415103a075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Apply Agglomerative Clustering\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=4)\n",
    "labels = agg_clustering.fit_predict(X)\n",
    "\n",
    "# Plot the hierarchical clustering result\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
    "plt.title(\"Agglomerative Clustering\")\n",
    "plt.show()\n",
    "# This method also finds 4 clusters but uses a different approach than K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f300d111-05aa-4738-a169-a88b3094ba71",
   "metadata": {},
   "source": [
    "# 7.2. Dimensionality Reduction\n",
    "Dimensionality reduction techniques aim to reduce the number of features in the data while preserving as much information as possible. These techniques are especially useful when dealing with high-dimensional data (many features) that may be sparse or redundant.\n",
    "\n",
    "## 7.2.1. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a technique for reducing the dimensionality of data by projecting it onto fewer dimensions (principal components) that explain the most variance in the data.\n",
    "\n",
    "**How it works:** PCA identifies the directions (principal components) along which the data varies the most and projects the data onto these directions. The first few principal components typically retain most of the information.\n",
    "\n",
    "**Key Concept:** PCA finds the eigenvectors and eigenvalues of the data's covariance matrix. The eigenvectors determine the direction of the new feature axes, and the eigenvalues determine their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad1b27-7e8b-450c-b263-42f7e664f6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Reduce the data to 2 dimensions using PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the data in 2D\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1])\n",
    "plt.title(\"PCA - Dimensionality Reduction\")\n",
    "plt.show()\n",
    "# This example reduces the data to two dimensions and projects it onto the two principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463ca603-f4dc-45fc-9f61-94fe64e712dd",
   "metadata": {},
   "source": [
    "## 7.2.2. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "t-SNE is a non-linear dimensionality reduction technique used for visualizing high-dimensional data in 2D or 3D. It is particularly good for visualizing clusters.\n",
    "\n",
    "**How it works:** t-SNE minimizes the divergence between probability distributions that measure pairwise similarities in the high-dimensional and low-dimensional spaces.\n",
    "\n",
    "**Key Concept:** Unlike PCA, which focuses on variance, t-SNE focuses on preserving local structure, making it better at capturing clusters. t-SNE can often reveal more visually distinct clusters compared to PCA, especially in cases where the data has complex relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7325e79-9ecd-482d-b2fb-1a0f41b2d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Plot the 2D data after t-SNE\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.title(\"t-SNE Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb37144-5906-41e9-ae06-6d313a163ead",
   "metadata": {},
   "source": [
    "# 7.3. Anomaly Detection\n",
    "Anomaly detection identifies outliers or unusual data points that do not conform to expected patterns.\n",
    "\n",
    "**Isolation Forest:** Isolation Forest is an unsupervised algorithm specifically designed for anomaly detection. It isolates outliers by randomly selecting a feature and then randomly selecting a split value between the minimum and maximum values of that feature.\n",
    "\n",
    "**How it works:** The idea is that anomalies are easier to isolate because they are far from the majority of the data points. The algorithm builds a forest of trees where outliers are isolated faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc150260-213b-4664-9ae0-2620206ec333",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Generate some synthetic data with outliers\n",
    "X_outliers = np.vstack([X, np.random.uniform(low=-10, high=10, size=(10, 2))])\n",
    "\n",
    "# Fit Isolation Forest model\n",
    "model = IsolationForest()\n",
    "outliers = model.fit_predict(X_outliers)\n",
    "\n",
    "# Plot data points and highlight outliers\n",
    "plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c=outliers, cmap='coolwarm')\n",
    "plt.title(\"Isolation Forest - Anomaly Detection\")\n",
    "plt.show()\n",
    "# the data points that are far from the main cluster (outliers) are flagged as anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec7a93-5f4f-4dcc-8602-bc3e7b0ffe21",
   "metadata": {},
   "source": [
    "# 7.4. Association Rule Learning\n",
    "Association rule learning is used to discover interesting relationships or patterns in large datasets, typically in market basket analysis.\n",
    "\n",
    "**Apriori Algorithm:** The Apriori algorithm is used to mine frequent itemsets and generate association rules. It’s widely used for market basket analysis to find relationships between products purchased together.\n",
    "\n",
    "**How it works:** The algorithm finds all itemsets that appear frequently together in the dataset and then generates rules like “If a customer buys bread, they are likely to also buy butter.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f4c995-d30c-4ad0-9c88-f0056f2f46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset: transactions with items\n",
    "data = {'Bread': [1, 1, 1, 0, 1],\n",
    "        'Butter': [1, 1, 0, 1, 1],\n",
    "        'Jam': [0, 1, 1, 0, 1]}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "print(rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
