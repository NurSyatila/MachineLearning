{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da63ebc-47c9-43d7-a1ee-e04fa666999e",
   "metadata": {},
   "source": [
    "# 1.0. Introduction to Machine Learning\n",
    "\n",
    "**Learning Objectives:** By the end of this lesson, you should be able to:\n",
    "\n",
    "* Understand what Machine Learning (ML) is and how it differs from traditional programming.\n",
    "* Identify and describe the three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.\n",
    "* Understand key ML terms such as features, labels, training, and testing data.\n",
    "* Apply a basic machine learning algorithm using a Python library like scikit-learn.\n",
    "  \n",
    "**Machine Learning, Deep Learning and Artificial Intelligence**\n",
    "The terms **Machine Learning (ML)**, **Deep Learning (DL)**, and **Artificial Intelligence (AI)** are often used interchangeably, but they represent different concepts in the field of computer science. Understanding the distinctions between them is crucial for understanding how they relate to one another.\n",
    "\n",
    "**Artificial Intelligence**\n",
    "AI refers to the broader concept of machines or software systems that can perform tasks that would normally require human intelligence. This includes tasks such as reasoning, learning, problem-solving, language understanding, and perception. AI covers a wide range of techniques, including both Machine Learning and Deep Learning.\n",
    "\n",
    "**Machine Learning**\n",
    "Machine Learning (ML) is a subset of AI that focuses on creating systems that can learn patterns and insights from large datasets. \n",
    "* Machine Learning refers to a field of study that allows computers to learn from data and make predictions or decisions without being explicitly programmed.\n",
    "* Traditional programming: You write rules to instruct the computer on how to solve a problem (e.g., sorting a list of numbers).\n",
    "* Machine learning: You provide data, and the computer identifies patterns in the data to make predictions or decisions (e.g., identifying whether an email is spam based on past examples).\n",
    "\n",
    "## 1.1. Key Concepts in Machine Learning\n",
    "* **Model:** A machine learning algorithm that learns patterns from the data and can make predictions based on those patterns.\n",
    "* **Training Data:** A subset of the data used to train the model. The model \"learns\" from this data.\n",
    "* **Testing Data:** A separate subset of the data used to evaluate how well the trained model performs on new, unseen data.\n",
    "* **Features:** Independent variables that are used as inputs to make predictions (e.g., in a house price prediction model, features might include the number of rooms, location, size of the house).\n",
    "* **Labels:** The dependent variable (target or outcome) that the model is trying to predict (e.g., the price of the house).\n",
    "\n",
    "## 1.2. Types of Learning\n",
    "ML includes various types of learning:\n",
    "\n",
    "### 1.2.1. Supervised Learning\n",
    "\n",
    "In supervised learning, we train the model on a labeled dataset. The algorithm learns from the input-output pairs and makes predictions on new, unseen data. \n",
    "\n",
    "**Examples:**\n",
    "* Classification: Predicting categories or classes (e.g., spam vs. not spam).\n",
    "* Regression: Predicting continuous values (e.g., predicting house prices).\n",
    "\n",
    "### 1.2.2. Unsupervised Learning \n",
    "\n",
    "In unsupervised learning, the model works with data that does not have labels. The goal is to find hidden patterns or groupings in the data.\n",
    "\n",
    "**Examples:**\n",
    "* Clustering: Grouping data points into clusters (e.g., customer segmentation).\n",
    "* Dimensionality Reduction: Reducing the number of features while retaining important information (e.g., PCA).\n",
    "\n",
    "### 1.2.3. Reinforcement Learning:\n",
    "\n",
    "Reinforcement learning is where an agent learns to make decisions by interacting with an environment. The agent gets rewards or penalties based on the actions it takes, and over time, it learns the best strategy to maximize rewards.\n",
    "\n",
    "**Examples:** \n",
    "* A self-driving car learning how to navigate streets safely.\n",
    "* Training an AI to play a game.\n",
    "\n",
    "## 1.3. Steps in Building a Machine Learning Model\n",
    "1. **Define the Problem**\n",
    "* Objective: Clearly define the problem you are trying to solve. Is it a classification, regression, or clustering problem? Do you want to predict labels, numerical values, or group data points into clusters?\n",
    "* Output: Understand what the expected output should look like (e.g., a category label, a continuous number, or a probability).\n",
    "* Evaluation Metric: Choose the appropriate metric(s) for model evaluation. Examples:\n",
    "  - For classification: accuracy, precision, recall, F1-score.\n",
    "  - For regression: mean squared error (MSE), mean absolute error (MAE).\n",
    "  - For clustering: silhouette score, adjusted Rand index.\n",
    "* Example: You may want to build a model to predict whether an email is spam or not, which is a binary classification problem, and the evaluation metric would be accuracy or F1-score.\n",
    "\n",
    "2. **Collect and Prepare the Data**\n",
    "\n",
    "* Data Collection: Gather the relevant dataset for your task. This could come from various sources like databases, APIs, sensors, or publicly available datasets (e.g., Kaggle, UCI).\n",
    "* Data Cleaning: Handle missing data, duplicates, or errors in the dataset.\n",
    "  - Missing Data: You can impute missing values or remove rows/columns with missing data.\n",
    "  - Outliers: Identify and handle outliers appropriately to avoid skewing results.\n",
    "* Feature Engineering: Select, transform, or create new features from raw data to improve the model’s performance.\n",
    "  - Encoding Categorical Variables: Use techniques like one-hot encoding or label encoding for categorical data.\n",
    "  - Normalization/Standardization: Scale features (e.g., min-max scaling or z-score normalization) to ensure they are on similar ranges, especially for algorithms like SVM and KNN.\n",
    "  - Feature Selection: Remove redundant or irrelevant features.\n",
    "* Data Splitting: Split the data into training, validation, and test sets (typically 70-80% for training, 10-15% for validation, and 10-15% for testing).\n",
    "\n",
    "Example: For a spam email classifier, you would collect a dataset of labeled emails, clean the text (remove stop words, punctuation), and transform the text into numerical features (e.g., using TF-IDF vectorization).\n",
    "\n",
    "3. **Choose a Model**\n",
    "* Select the Right Algorithm: Choose the type of machine learning model based on the problem:\n",
    "  - Supervised Learning:\n",
    "    - Classification: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forest, k-NN, Neural Networks.\n",
    "    - Regression: Linear Regression, Lasso, Ridge, Support Vector Regression, Random Forest Regression, etc.\n",
    "  - Unsupervised Learning:\n",
    "    - Clustering: k-Means, DBSCAN, Agglomerative Clustering.\n",
    "    - Dimensionality Reduction: PCA, t-SNE.\n",
    "    - Reinforcement Learning: Q-learning, Deep Q-Networks (DQN).\n",
    "\n",
    "Consider the nature of your data, the interpretability of the model, and the trade-offs between model complexity and performance.\n",
    "\n",
    "Example: For classifying emails into spam or not, you might start with a simple Logistic Regression or Random Forest model for binary classification.\n",
    "\n",
    "4. **Train the Model**\n",
    "* Model Training: Train the model on the training dataset. During training, the algorithm learns patterns from the data by adjusting its internal parameters (e.g., weights in a linear regression model or decision thresholds in a decision tree).\n",
    "* Hyperparameter Tuning: Tune the hyperparameters of the model (e.g., learning rate, number of trees in a random forest, regularization strength). This is often done using techniques like Grid Search or Random Search.\n",
    "  - Grid Search: Exhaustively search over a range of hyperparameter values.\n",
    "  - Random Search: Randomly sample hyperparameters to find a good configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d7e1f1-7426-44f6-a623-354104e6cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4519726-eaf4-4ad3-bdeb-6add1eeca327",
   "metadata": {},
   "source": [
    "5. **Evaluate the Model**\n",
    "* Test on Validation Data: Use the validation set to tune hyperparameters and assess the model’s performance. This helps you adjust the model before evaluating it on the test set.\n",
    "* Performance Metrics: Use appropriate metrics to evaluate the model’s performance.\n",
    "  - For classification: Accuracy, precision, recall, F1-score, confusion matrix.\n",
    "  - For regression: MSE, MAE, R-squared.\n",
    "* Cross-Validation: Use k-fold cross-validation to assess the model’s performance on multiple data subsets to ensure that it generalizes well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b452060f-6177-47b2-98de-96bd8dfec1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = grid_search.predict(X_val)\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d20435a-6463-4e0f-a4c6-9b7da34c1d8c",
   "metadata": {},
   "source": [
    "6. **Tune the Model**\n",
    "* Hyperparameter Optimization: Based on the evaluation results, tune the hyperparameters further. You can also try different algorithms and compare their performance.\n",
    "* Feature Engineering: Add, remove, or transform features based on insights from model evaluation.\n",
    "* Address Overfitting/Underfitting: Use techniques like cross-validation, regularization (L1, L2), or early stopping to prevent overfitting. For underfitting, consider using more complex models or adding more features.\n",
    "\n",
    "Example: If your model is overfitting, you might add regularization to the logistic regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45cf598-bfc5-4904-b671-61b4e584d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=1.0)  # Try different values of C for regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f77a9c-91ce-4edc-a167-edd74ad745fb",
   "metadata": {},
   "source": [
    "7. **Test the Model**\n",
    "* Evaluate on the Test Data: After the model has been trained and tuned using the training and validation sets, evaluate its final performance on the test dataset to simulate how the model will perform on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27502b61-8832-4c6f-9fa2-46a5506eee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = model.score(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b8449-f375-4f48-a04b-145281ced39c",
   "metadata": {},
   "source": [
    "8. **Deploy the Model**\n",
    "* Model Serialization: Save the trained model to a file for future use or deployment. This can be done using Pickle or Joblib (for Python models).\n",
    "* Deployment: Integrate the model into a production environment, such as an API or embedded system, so that it can make real-time predictions.\n",
    "* Monitor Performance: Monitor the model’s performance over time and retrain it periodically with new data if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc9825-c96e-4e99-b31a-2bc1aeceae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('spam_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6752ea-ffc5-43dd-90f7-1a976e386857",
   "metadata": {},
   "source": [
    "9. **Maintain and Update the Model**\n",
    "\n",
    "* Retraining: As new data becomes available, retrain the model to keep it up-to-date and improve its predictions.\n",
    "* Monitoring: Continuously monitor the model’s performance in production to detect any performance degradation (e.g., model drift).\n",
    "* Model Reassessment: Regularly reassess whether the model is still suitable for the problem, and update it as needed.\n",
    "\n",
    "**Example Workflow in Python using scikit-learn (Classification Task):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d8607-b09b-44b7-9217-c7451d84813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Load Data\n",
    "data = pd.read_csv(\"data.csv\")\n",
    "X = data.drop('target', axis=1)  # Features\n",
    "y = data['target']  # Target variable\n",
    "\n",
    "# Step 2: Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Choose a Model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "# Step 6: Save the Model\n",
    "import pickle\n",
    "with open('random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa369b8-4082-49ce-b521-1381abfdcc1c",
   "metadata": {},
   "source": [
    "**Example Workflow in Python using scikit-learn (Regression Task):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023a68b0-2708-46c5-b23a-239c2e09fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "# Step 1: Load the data\n",
    "# Example: loading a CSV dataset\n",
    "data = pd.read_csv(\"data.csv\")  # Assuming 'data.csv' has columns ['feature1', 'feature2', ..., 'target']\n",
    "\n",
    "# Assume the target variable is in the 'target' column and features are all other columns\n",
    "X = data.drop(columns='target')  # Features (all columns except 'target')\n",
    "y = data['target']  # Target variable\n",
    "\n",
    "# Step 2: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Choose the model (Linear Regression)\n",
    "model = LinearRegression()\n",
    "\n",
    "# Step 4: Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
    "\n",
    "# Step 6: Save the trained model to a file for future use\n",
    "with open('linear_regression_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Step 7: (Optional) Load the saved model and make predictions\n",
    "with open('linear_regression_model.pkl', 'rb') as model_file:\n",
    "    loaded_model = pickle.load(model_file)\n",
    "\n",
    "# Making predictions with the loaded model\n",
    "y_loaded_pred = loaded_model.predict(X_test)\n",
    "print(f\"Predictions from Loaded Model: {y_loaded_pred[:5]}\")  # Show first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d8017b-37ed-4715-b20f-5b212d289f67",
   "metadata": {},
   "source": [
    "## 1.4. Example: Building a Simple Machine Learning Model\n",
    "Now, let's walk through an example of building a machine learning model using scikit-learn, one of the most popular Python libraries for machine learning. We’ll use a simple classification problem: predicting whether a person has diabetes based on features such as age, BMI, and insulin levels.\n",
    "\n",
    "Example: For a spam email classifier, you would collect a dataset of labeled emails, clean the text (remove stop words, punctuation), and transform the text into numerical features (e.g., using TF-IDF vectorization).\n",
    "**Step-by-Step Code Walkthrough:**\n",
    "\n",
    "**1. Import necessary libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d08e92f-db4d-4966-9015-81818016ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0bcd2-942e-401a-8dfe-9979656d950d",
   "metadata": {},
   "source": [
    "**2. Load the dataset:** You can use a dataset like the famous Pima Indians Diabetes Dataset, which is available publicly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f0527e-d302-4e7b-9aae-69120df2d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "data = pd.read_csv(url, names=columns)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6db00d-e18b-4b6e-b2b9-3b5a1fed6c5e",
   "metadata": {},
   "source": [
    "**3. Split the dataset into training and testing data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ed124-424c-41d8-a860-c1f3f1f33f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X) and Labels (y)\n",
    "X = data.drop('Outcome', axis=1)  # Drop the label column\n",
    "y = data['Outcome']  # Target variable\n",
    "\n",
    "# Split the data into training and testing sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc3894-103a-480a-ba79-6c0c1c2fb9cb",
   "metadata": {},
   "source": [
    "**4. Train a logistic regression model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdd9570-2c0c-4fdf-9f3e-fb86bf55f95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model using the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b2a85-fb53-413b-b260-a8dc9fc074d8",
   "metadata": {},
   "source": [
    "**5. Make predictions and evaluate the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e22fce9-430c-4931-8762-477cfb3e5683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the testing data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model by calculating accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80553258-1ee4-4cf2-bfa1-283c6816b217",
   "metadata": {},
   "source": [
    "This is a simple example of using supervised learning (classification) to predict a binary outcome (diabetes or not). In this case, the logistic regression model is trained on the training data, evaluated on the test data, and we measure how well it predicts the outcomes.\n",
    "\n",
    "**Homework:** Try building a machine learning model using a different dataset (e.g., the Iris dataset or Wine dataset) and apply a different algorithm such as K-Nearest Neighbors or Decision Trees.\n",
    "\n",
    "**Next Lesson:** Dive deeper into specific algorithms like linear regression, decision trees, or support vector machines, and how to fine-tune these models.\n",
    "\n",
    "**Resources:**\n",
    "* https://machinelearningmastery.com/\n",
    "* https://www.kaggle.com/ for datasets and practice problems\n",
    "* Scikit-learn documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
