{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbfbbb1-695a-408b-a1e2-7e835b26aea9",
   "metadata": {},
   "source": [
    "# 5.0 Data Preprocessing and Feature Engineering\n",
    "The lesson will cover key aspects of data transformation, including techniques that are crucial in preparing data for machine learning models and improving their performance.\n",
    "\n",
    "**Lesson objectives:** By the end of this lesson, students should be able to\n",
    "* Understand the concepts of data preprocessing and feature engineering\n",
    "* Apply data scaling techniques using Python\n",
    "* Evaluate the impact of scaling on model performance\n",
    "* Understand the need for scaling in feature engineering\n",
    "\n",
    "Data preprocessing and feature engineering are essential steps in any machine learning pipeline because real-world data is often messy, incomplete, and not in a format that models can easily process.\n",
    "\n",
    "**Data Preprocessing**\n",
    "* The process of cleaning and preparing data before feeding it into a machine learning model.\n",
    "* Key steps: handling missing values, encoding categorical variables, scaling/normalizing numerical features, and splitting data into training and test sets.\n",
    "\n",
    "**Feature Engineering**\n",
    "* The process of transforming raw data into meaningful features that improve model performance.\n",
    "* Feature engineering helps improve the predictive power of machine learning algorithms by making the data more understandable or relevant to the model.\n",
    "* Includes creating new features, encoding categorical features, and applying domain knowledge to the data.\n",
    "\n",
    "# 5.1. Data Cleaning\n",
    "## 5.1.1 Handling Missing Data\n",
    "Missing data can occur due to various reasons such as errors during data collection or loss of data over time.\n",
    "**Methods for Handling Missing Data:**\n",
    "* **1) Removing Missing Data using `.dropna()`:** If a column or row has too many missing values, it might be better to drop it. Pandas functions like `.dropna()` can be used to remove missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455da94-3e14-40ee-91dc-9c2e152edc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset with missing values\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob', np.nan, 'Eve'],\n",
    "    'Age': [25, np.nan, 22, 28, 29],\n",
    "    'City': ['New York', 'Los Angeles', np.nan, 'Chicago', 'Houston']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e4b4c-0775-44f7-bf49-1129c08e454c",
   "metadata": {},
   "source": [
    "**a) Remove Rows with Any Missing Values using `dropna()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6339e904-8659-4acb-82f6-a98441570446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any missing values\n",
    "df_no_na_rows = df.dropna()\n",
    "\n",
    "print(\"DataFrame After Removing Rows with Any Missing Values:\")\n",
    "print(df_no_na_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88efc4db-3945-4d1f-98e5-6232b6915365",
   "metadata": {},
   "source": [
    "**b) Remove Columns with Any Missing Values using `dropna()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2eb68-5fcc-42aa-be0c-3a9beddb4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns with any missing values\n",
    "df_no_na_columns = df.dropna(axis=1)\n",
    "\n",
    "print(\"DataFrame After Removing Columns with Any Missing Values:\")\n",
    "print(df_no_na_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc784a3-0f7b-4a7a-8121-72b5680220d2",
   "metadata": {},
   "source": [
    "**c) Remove Rows with Missing Values in Specific Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7a4fb-8620-4c48-b3c4-18e1556de574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'Age' column has missing values\n",
    "df_no_na_age = df.dropna(subset=['Age'])\n",
    "\n",
    "print(\"DataFrame After Removing Rows with Missing 'Age':\")\n",
    "print(df_no_na_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64041f4a-c9fc-4137-91c8-3772d121ef02",
   "metadata": {},
   "source": [
    "* **2) Inputation using `.fillna()`:**\n",
    "Instead of removing data, you can impute missing values, i.e., fill them in with some reasonable substitute value. The most common methods for imputation are:\n",
    "  - Mean/Median/Mode Imputation: Replace missing values with the mean (numerical features) or mode (categorical features) of the column.\n",
    "  - Forward/Backward Fill: For time-series data, you can forward fill or backward fill missing values.\n",
    "  - Advanced Imputation: Use models like KNN imputation or Multivariate Imputation by Chained Equations (MICE) for more sophisticated handling.\n",
    "\n",
    "**a) Fill Missing Data with a Specific Value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507a1d56-334f-46c5-8af1-79097ed79885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'Age' with the mean of the column\n",
    "df_imputed_age = df.fillna({'Age': df['Age'].mean()})\n",
    "\n",
    "print(\"DataFrame After Imputing Missing 'Age' with Mean:\")\n",
    "print(df_imputed_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c6511-ea70-478a-99b5-0c68a0e6b292",
   "metadata": {},
   "source": [
    "**b) Fill Missing Data with Mode for Categorical Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7333c-e755-41d8-a74f-f8050d0e2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 'City' with the mode (most frequent value)\n",
    "df_imputed_city = df.fillna({'City': df['City'].mode()[0]})\n",
    "\n",
    "print(\"DataFrame After Imputing Missing 'City' with Mode:\")\n",
    "print(df_imputed_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a21fbc-58da-4160-870a-6f8252a093a8",
   "metadata": {},
   "source": [
    "**c) Forward Fill or Backward Fill**\n",
    "For time-series data or datasets where you expect adjacent rows to have similar values, you might use:\n",
    "* forward fill (`ffill()`): Fills the missing value with the previous row’s value\n",
    "* backward fill (`bfill()`): Fills the missing value with the previous row’s value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6273f-b4e7-4d26-82d5-42a8c0742fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill missing data\n",
    "df_forward_fill = df.fillna(method='ffill')\n",
    "\n",
    "print(\"DataFrame After Forward Filling Missing Values:\")\n",
    "print(df_forward_fill)\n",
    "\n",
    "# Backward fill missing data\n",
    "df_backward_fill = df.fillna(method='bfill')\n",
    "\n",
    "print(\"DataFrame After Backward Filling Missing Values:\")\n",
    "print(df_backward_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2bb45d-20dc-4125-a875-c569bc3b1a51",
   "metadata": {},
   "source": [
    "## 5.1.2. Removing duplicates\n",
    "Duplicate data can lead to inaccurate models, biased results, and wasted computational resources. Duplicate entries in your dataset can occur for various reasons, such as errors during data collection, merging datasets from different sources, or incorrect data entry. Cleaning these duplicates helps ensure the integrity and reliability of your analysis or machine learning model.\n",
    "\n",
    "**Methods for Removing Duplicates**\n",
    "\n",
    "You can use the duplicated() method to identify duplicate rows in your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5130e77-b955-41a0-9bb1-495bac586ce6",
   "metadata": {},
   "source": [
    "**1) Identify duplicates:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1d3414-c710-41bf-8f0a-ad2ba0c358a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b11cf6-4f02-4fe6-8b29-41a494c57993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5, 3],\n",
    "    'Name': ['John', 'Alice', 'Bob', 'Alice', 'Eve', 'Bob'],\n",
    "    'Age': [25, 30, 22, 30, 29, 22]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Identify duplicates\n",
    "duplicates = df[df.duplicated()]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7040ab9-187e-44b9-8f91-0ff34249cf83",
   "metadata": {},
   "source": [
    "**2) Removing Duplicates in Pandas using `drop_duplicates()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315a830-0efb-4f35-a088-e80838b90f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates (by default, keeps the first occurrence of each duplicate row)\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "print(df_no_duplicates)\n",
    "# or remove duplicates and keep the last occurrence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4256a4e-ed86-45f4-8382-bd6ffa75e2a0",
   "metadata": {},
   "source": [
    "**3) Removing Duplicates Based on Specific Columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f60b8a-60b1-470f-a661-790808aa36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates based on the 'Name' column\n",
    "df_no_duplicates_name = df.drop_duplicates(subset=['Name'])\n",
    "print(df_no_duplicates_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49d5b30-7ab5-4b4b-848d-0e8c8ef22bdf",
   "metadata": {},
   "source": [
    "### 5.1.3. Outlier detection and removal\n",
    "Outliers are data points that significantly differ from the rest of the data. They can result from measurement errors, data entry errors, or can represent genuine but rare events. Whether they should be removed or not depends on the context and the goal of your analysis or machine learning task.\n",
    "\n",
    "Outlier detection is crucial because:\n",
    "* Outliers can negatively impact models that are sensitive to extreme values, like linear regression, K-means clustering, or principal component analysis (PCA).\n",
    "* Outliers can distort statistical summaries such as the mean and standard deviation.\n",
    "* Some models (like random forests and tree-based models) are more robust to outliers but still may benefit from detecting and treating them.\n",
    "\n",
    "There are several methods for detecting and removing outliers, each suitable for different data types and use cases. \n",
    "\n",
    "**Common techniques for outlier detection and removal**\n",
    "\n",
    "**1) Z-Score (Standard Score)**\n",
    "The Z-score represents how many standard deviations a data point is away from the mean. A high absolute value of the Z-score indicates that the data point is far from the mean and may be an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378e360-bf41-4ac1-a766-2de4bffa73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Example dataset\n",
    "data = {'Value': [10, 12, 12, 13, 14, 15, 16, 16, 100, 18, 19, 20]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Calculate Z-scores for each data point\n",
    "df['Z-Score'] = zscore(df['Value'])\n",
    "\n",
    "# Display Z-scores and filter outliers (Z-score > 3 or < -3)\n",
    "print(df)\n",
    "\n",
    "# Remove outliers (Z-score > 3 or < -3)\n",
    "df_no_outliers = df[df['Z-Score'].abs() <= 3]\n",
    "\n",
    "print(\"\\nData after Removing Outliers:\")\n",
    "print(df_no_outliers)\n",
    "# In this example, the value 100 had a Z-score of 7.21, \n",
    "# which is much larger than the threshold of 3, so it was removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e4c2e2-bba7-49f0-a1c2-58aaf7458df1",
   "metadata": {},
   "source": [
    "**2) IQR (Interquartile Range) Method**\n",
    "The IQR method is another popular technique for detecting outliers. The IQR is the range between the first quartile (Q1) and the third quartile (Q3). \n",
    "Outliers are typically defined as:\n",
    "* Lower bound: 𝑄1 − 1.5 × IQR\n",
    "* Upper bound: Q1 − 1.5 × IQR\n",
    "  \n",
    "Any data points below the lower bound or above the upper bound are considered outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e414c564-6521-4200-9ff8-eed75ce64992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR for the 'Value' column\n",
    "Q1 = df['Value'].quantile(0.25)\n",
    "Q3 = df['Value'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"IQR: {IQR}\")\n",
    "print(f\"Lower Bound: {lower_bound}\")\n",
    "print(f\"Upper Bound: {upper_bound}\")\n",
    "\n",
    "# Filter out outliers based on IQR\n",
    "df_no_outliers_iqr = df[(df['Value'] >= lower_bound) & (df['Value'] <= upper_bound)]\n",
    "\n",
    "print(\"\\nData after Removing Outliers (IQR):\")\n",
    "print(df_no_outliers_iqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179523a-1562-442e-98c0-e4cc9b3e7b9c",
   "metadata": {},
   "source": [
    "# 5.2. Data Transformation\n",
    "Data transformation is a key step in preprocessing for machine learning. It helps convert raw data into a more useful format for model training, improving the model’s performance and efficiency.\n",
    "\n",
    "## 5.2.1. Normalization, Standardization, and Scaling\n",
    "These techniques are used to adjust the range and distribution of numerical features in your dataset, which can be crucial for algorithms that are sensitive to the scale of input features (like SVM, KNN, and Gradient Descent-based algorithms).\n",
    "\n",
    "### 5.2.1.1. Normalization (Min-Max Scaling)\n",
    "Normalization (also known as Min-Max Scaling) transforms the data to fit within a specific range, typically [0, 1], or [-1, 1]. Normalization is particularly useful when the data follows a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485e261-578d-4671-bb29-c331fe1395ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = {'Feature1': [1, 5, 10, 15, 20], 'Feature2': [100, 200, 300, 400, 500]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Normalize the data\n",
    "df_normalized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"Normalized DataFrame:\")\n",
    "print(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d66617-7698-45db-90c2-3e79f2652832",
   "metadata": {},
   "source": [
    "### 5.2.1.2. Standardization (Z-Score Normalization)\n",
    "Standardization transforms the data to have a mean of 0 and a standard deviation of 1. It’s useful when your data follows a Gaussian (normal) distribution or when you want to compare features that have different units or scales. Standardization is often the preferred method for algorithms that assume a normal distribution (e.g., Linear Regression, Logistic Regression, SVM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9185a8-eaff-4e50-bab4-215cb660c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the data\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"Standardized DataFrame:\")\n",
    "print(df_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23353410-fb32-4a6d-93b4-62e3d1138580",
   "metadata": {},
   "source": [
    "### 5.2.1.3. Scaling (Robust Scaling)\n",
    "Scaling can also refer to Robust Scaling, which is less sensitive to outliers. It scales the data based on the median and interquartile range (IQR) rather than the mean and standard deviation, making it more robust to extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796611a-c85f-4ace-8286-eec001ec88ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Initialize the RobustScaler\n",
    "scaler = RobustScaler()\n",
    "\n",
    "# Scale the data\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(\"Scaled DataFrame (Robust Scaling):\")\n",
    "print(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3c860-166a-4987-b879-4250195e437c",
   "metadata": {},
   "source": [
    "### 5.2.2. Handling Categorical Data\n",
    "Many machine learning models require numerical data, but real-world datasets often contain categorical variables like \"Gender\", \"City\", or \"Category\". To convert these into a usable format, it is necessary to encode them.\n",
    "\n",
    "**Methods for Encoding Categorical Variables:**\n",
    "#### 5.2.2.1. One-Hot Encoding\n",
    "One-Hot Encoding creates binary columns for each category in a categorical feature. It’s useful when the feature is nominal (categories have no meaningful order).\n",
    "\n",
    "For example, a \"Color\" feature with values \"Red\", \"Green\", and \"Blue\" will be converted into three binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a6a75-eb71-42f7-9b55-afeeec930785",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Color': ['Red', 'Blue', 'Green', 'Blue', 'Red']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# One-hot encode the 'Color' column\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoded_data = encoder.fit_transform(df[['Color']])\n",
    "\n",
    "# Convert the result into a DataFrame\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.categories_[0])\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cafad3-420e-4d1e-97df-f1df3d0ed69f",
   "metadata": {},
   "source": [
    "#### 5.2.2.2. Label Encoding\n",
    "Label Encoding assigns each category a unique integer. This is useful for ordinal features (categories with a meaningful order), such as \"Low\", \"Medium\", \"High\". However, this method is not suitable for nominal data because it introduces a false order between categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092a50bd-7393-437b-ab35-9334b3d7c085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample data\n",
    "data = {'Size': ['Small', 'Medium', 'Large', 'Medium', 'Small']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Perform Label Encoding\n",
    "df['Size_Encoded'] = encoder.fit_transform(df['Size'])\n",
    "\n",
    "print(\"Label Encoded DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101537f1-ad1f-4344-ad45-8bd7106de36b",
   "metadata": {},
   "source": [
    "#### 5.2.2.3. Ordinal Encoding\n",
    "Ordinal Encoding is a form of label encoding where categories are ordered in a meaningful way, such as \"Low\", \"Medium\", \"High\". This method maps each category to an integer value, preserving the ordinal relationship between categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d02fe-7cbe-41c7-bd69-bdd701836059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Example data\n",
    "data = {'Rating': ['Low', 'Medium', 'High', 'Medium', 'Low']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
    "\n",
    "# Apply ordinal encoding\n",
    "df['Rating_encoded'] = ordinal_encoder.fit_transform(df[['Rating']])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e3ad3-4c07-4a52-ba75-eb5f4b12ff2c",
   "metadata": {},
   "source": [
    "#### 5.2.2.4. Binary Encoding\n",
    "Binary Encoding is a combination of label encoding and one-hot encoding. It first converts the category labels into integers (like label encoding), then converts those integers into their binary equivalents. Binary encoding is useful for handling categorical variables with many levels. It is useful when there are a large number of categories in a categorical variable and you want to avoid the high dimensionality problem that one-hot encoding causes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c2cef8-b542-4a72-a24e-6f3151bf47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {'Category': ['A', 'B', 'C', 'A', 'B', 'C']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize BinaryEncoder\n",
    "encoder = ce.BinaryEncoder(cols=['Category'])\n",
    "\n",
    "# Apply binary encoding\n",
    "df_encoded = encoder.fit_transform(df)\n",
    "\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97c622-9d75-48a8-905a-fd35619ff694",
   "metadata": {},
   "source": [
    "#### 5.2.2.5. Target Encoding (Mean Encoding)\n",
    "Target Encoding replaces the category values with the mean of the target variable for each category. This method is particularly useful for high-cardinality categorical variables (i.e., when there are many unique categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bd399e-3804-4a27-9c52-7b2ae0a80b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "import pandas as pd\n",
    "\n",
    "# Example data\n",
    "data = {'Category': ['A', 'B', 'A', 'B', 'A'],\n",
    "        'Target': [1, 0, 1, 0, 1]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize TargetEncoder\n",
    "encoder = ce.TargetEncoder(cols=['Category'])\n",
    "\n",
    "# Apply target encoding\n",
    "df_encoded = encoder.fit_transform(df['Category'], df['Target'])\n",
    "\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb4f204-6b2e-43ab-8bfe-c3bcb4bd3a69",
   "metadata": {},
   "source": [
    "### 5.2.3. Feature extraction and feature selection\n",
    "Feature extraction and feature selection are techniques used to improve model performance by reducing the number of irrelevant or redundant features.\n",
    "\n",
    "#### 5.2.3.1. Feature Extraction\n",
    "Feature extraction is the process of transforming raw data into a set of usable features for machine learning. It’s often used with unstructured data like images, text, or time series.\n",
    "* For text data, feature extraction methods like TF-IDF (Term Frequency-Inverse Document Frequency) and Word2Vec are commonly used.\n",
    "* For image data, feature extraction methods like HOG (Histogram of Oriented Gradients) or CNNs (Convolutional Neural Networks) can be applied.\n",
    "\n",
    "**Example of Feature Extraction for Text Data (TF-IDF):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a74345-7455-4038-8489-8afb40e1c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "texts = [\"I love machine learning\", \"Machine learning is great\", \"I love coding\"]\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the data into feature vectors\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Convert the result into a DataFrame for better visualization\n",
    "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Feature Extraction with TF-IDF:\")\n",
    "print(df_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33f90c-c53a-43e0-848a-e985ac87b3e9",
   "metadata": {},
   "source": [
    "#### 5.2.3.2. Feature Selection\n",
    "Feature selection is the process of choosing the most relevant features for the model, while removing irrelevant or redundant features.\n",
    "Common feature selection techniques include:\n",
    "* Filter methods (e.g., Correlation matrix, Chi-square test)\n",
    "* Wrapper methods (e.g., Recursive Feature Elimination (RFE))\n",
    "* Embedded methods (e.g., Lasso regression, Decision Trees)\n",
    "\n",
    "**Example of Feature Selection using Recursive Feature Elimination (RFE):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc59c5c-6518-4c9e-bc41-b1dca37a36fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sample data (X = features, y = target)\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Initialize the model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "\n",
    "# Initialize RFE and select top 2 features\n",
    "rfe = RFE(model, 2)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "print(\"Selected Features based on RFE:\")\n",
    "print(rfe.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadec495-66e1-40d9-a05f-02874bbb55a6",
   "metadata": {},
   "source": [
    "## 5.3. Data Splits\n",
    "In machine learning, it is essential to split the data into different subsets to avoid overfitting and assess the model's generalizability to unseen data.\n",
    "\n",
    "* **Training Set:** The portion of the data used to train the machine learning model.\n",
    "* **Test Set:** The portion of the data used to evaluate the performance of the trained model on unseen data.\n",
    "* **Validation Set:** An optional dataset used during model training for tuning hyperparameters. It’s typically used in combination with techniques like cross-validation.\n",
    "\n",
    "The key goal of splitting the data is to simulate how the model will perform on real-world, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805e452-3bbe-41bc-bf9c-d8f6c3bfcfbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a0e34-2e8a-49cf-90d4-5c00088b349a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "652ced01-7a83-47f9-bba2-088d1cab1fbe",
   "metadata": {},
   "source": [
    "### 5.3.1. Train-Test \n",
    "The train-test split is the simplest and most common method of splitting a dataset. It involves dividing the data into two parts:\n",
    "\n",
    "* A training set (typically 70-80% of the data).\n",
    "* A test set (typically 20-30% of the data).\n",
    "\n",
    "The model is trained on the training set and evaluated on the test set. The splitting process should ideally be random to ensure that both the training and test sets are representative of the whole dataset. In Python, this can be done easily using Scikit-learn’s `train_test_split()`.\n",
    "\n",
    "**Example of Train-Test Split in Python:**\n",
    "In this example, the dataset is split into 80% for training and 20% for testing, ensuring that the model can be trained on most of the data and tested on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194bcea-c981-4f0d-bb75-20be901c81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'Feature1': [1, 2, 3, 4, 5, 6],\n",
    "    'Feature2': [10, 20, 30, 40, 50, 60],\n",
    "    'Target': [0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target variable\n",
    "X = df[['Feature1', 'Feature2']]  # Feature matrix\n",
    "y = df['Target']                  # Target variable\n",
    "\n",
    "# Split the data (80% for training and 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Output the shapes of the split data\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408d0ab2-11b9-43af-aef5-0b3b8dccc659",
   "metadata": {},
   "source": [
    "### 5.3.2. Cross-Validation\n",
    "Cross-validation is a more robust technique for evaluating machine learning models. It involves splitting the data into multiple subsets (folds), training the model on different combinations of these folds, and evaluating it on the remaining fold(s). The most common form is k-fold cross-validation.\n",
    "\n",
    "In k-fold cross-validation, the dataset is split into k equal-sized folds. For each fold:\n",
    "* The model is trained on k-1 folds.\n",
    "* The model is evaluated on the remaining fold.\n",
    "* This process is repeated k times, with each fold serving as the test set once.\n",
    "\n",
    "The advantage of cross-validation over a single train-test split is that it allows the model to be evaluated multiple times, using different parts of the data for training and testing. This leads to a more reliable estimate of the model’s performance.\n",
    "\n",
    "Several types of cross-validation methods are available, such as:\n",
    "* K-Fold Cross-Validation: The dataset is split into k subsets. Each fold serves as a validation set once.\n",
    "* Stratified k-Fold Cross-Validation: Used for classification tasks when the data is imbalanced. Ensures that each fold has the same proportion of class labels.\n",
    "* Leave-One-Out Cross-Validation (LOO CV): A special case where each fold contains only one data point. This is often used with very small datasets.\n",
    "\n",
    "In Python, cross-validation can be performed using Scikit-learn’s `cross_val_score()` or `KFold`.\n",
    "\n",
    "**Example of k-Fold Cross-Validation in Python:**\n",
    "In this example, the logistic regression model was evaluated using 3-fold cross-validation, and the mean of the cross-validation scores is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4b1dd0-14a5-4afb-bf79-f83e7ccf0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Create a simple dataset\n",
    "X = np.array([[1, 10], [2, 20], [3, 30], [4, 40], [5, 50], [6, 60]])\n",
    "y = np.array([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Initialize a logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=3)  # cv=3 means 3-fold cross-validation\n",
    "\n",
    "print(f\"Cross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV score: {np.mean(cv_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edebd7f6-8061-4b07-8ce7-9d71f938af98",
   "metadata": {},
   "source": [
    "**Homework/Exercise:**\n",
    "* Task 1: Implement normalization and standardization on a dataset and compare the performance of different models before and after scaling.\n",
    "* Task 2: Apply One-hot encoding and Label encoding to a categorical dataset and train a classification model to compare the results.\n",
    "* Task 3: Use Recursive Feature Elimination (RFE) and SelectKBest to perform feature selection on a dataset and compare the model’s performance with all features versus the selected features.\n",
    "* Task 4: For a dataset with missing values, demonstrate how imputation, feature extraction, and feature selection can be used together to prepare the dataset for machine learning.\n",
    "* Task 5: Use train-test split on a dataset of your choice and evaluate the model performance.\n",
    "* Task 6: Implement k-fold cross-validation using cross_val_score and compare the results with a single train-test split.\n",
    "* Task 7: If available, use a real-world dataset with imbalanced classes (e.g., fraud detection) and perform stratified k-fold cross-validation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
