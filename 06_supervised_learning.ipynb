{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bedf73a9-551c-4187-9a38-7f12faab92a2",
   "metadata": {},
   "source": [
    "# 6.0 Supervised learning algorithms\n",
    "This lesson provides an overview of several key supervised learning algorithms and their practical implementation.\n",
    "\n",
    "**Lesson Objectives:** By the end of the lesson, students should be able to:\n",
    "* Understand the basic concepts of supervised learning algorithms.\n",
    "* Implement key supervised learning algorithms using real-world datasets.\n",
    "\n",
    "**Supervised Learning** involves learning from labeled data to predict outcomes. The goal is to train a model that maps input data to a target (output) variable. Supervised learning algorithms are typically categorized based on the type of problem they aim to solve: regression or classification\n",
    "\n",
    "**Key Types of Supervised Learning:**\n",
    "* **Regression:** Predicting continuous outputs (e.g., house prices, stock prices).\n",
    "* **Classification:** Predicting categorical outputs (e.g., spam detection, image classification).\n",
    "\n",
    "# 6.1. Linear Regression\n",
    "\n",
    "**Key Concepts:**\n",
    "* **Simple Linear Regression:** Involves finding the line of best fit for a set of data points using the formula `y = mx + b` where *m* is the slope (coefficient), and *b* is the y-intercept.\n",
    "* **Multiple Linear Regression:** Extends simple linear regression to multiple predictors (features). The model is given by `y = w<sub>1</sub>x<sub>1</sub> +w<sub>2</sub>x<sub>1</sub> + ... + w<sub>n</sub> +b`\n",
    "* **Cost Function:** Mean Squared Error (MSE), which quantifies the difference between predicted and actual values.\n",
    "* **Gradient Descent:** The optimization technique used to minimize the cost function by iteratively adjusting model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e548a80b-3ffe-42a9-bbbd-273fd859a14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"True values\")\n",
    "plt.ylabel(\"Predicted values\")\n",
    "plt.title(\"Linear Regression: True vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7776e6c1-a40a-4f26-aef5-8a1e26b31034",
   "metadata": {},
   "source": [
    "# 6.2 Logistic Regression \n",
    "**Key Concepts:**\n",
    "* **Sigmoid Function:** Converts linear outputs to a range between 0 and 1, ideal for binary classification problems.\n",
    "* **Binary Classification:** Logistic regression is used to classify data into two categories (e.g., yes/no, 0/1).\n",
    "* **Model Evaluation Metrics:** For classification problems, accuracy is not always the best metric. We use:\n",
    "  - **Accuracy:** The proportion of correctly classified instances.\n",
    "  - **Precision:** The ratio of true positives to predicted positives.\n",
    "  - **Recall:** The ratio of true positives to actual positives.\n",
    "  - **F1-Score:** The harmonic mean of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab48069-e283-4a2e-8456-03254a4183fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = (data.target == 0).astype(int)  # Convert to binary (setosa or not)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d6117-001b-49c9-81bd-9d86fed44761",
   "metadata": {},
   "source": [
    "# 6.3.  Decision Trees and Random Forests\n",
    "**Key Concepts:**\n",
    "* **Decision Trees:** A model that splits data into subsets based on feature values, creating a tree-like structure. Gini Index and Entropy are criteria used to choose the best splits.\n",
    "* **Random Forests:** An ensemble method that combines multiple decision trees to improve accuracy and reduce overfitting. Bagging is a method of training multiple models (trees) on different random subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd81b6-7b97-4136-b357-86099d3f8b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train Decision Tree\n",
    "tree_model = DecisionTreeClassifier(random_state=42)\n",
    "tree_model.fit(X, y)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_tree = tree_model.predict(X)\n",
    "y_pred_rf = rf_model.predict(X)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Decision Tree Classification Report\")\n",
    "print(classification_report(y, y_pred_tree))\n",
    "\n",
    "print(\"Random Forest Classification Report\")\n",
    "print(classification_report(y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c9337-cfaf-4778-8fab-70d9b94fd620",
   "metadata": {},
   "source": [
    "# 6.4. Support Vector Machines (SVM)\n",
    "**Key Concepts:**\n",
    "* Hyperplanes and Margins: SVM tries to find the hyperplane that best separates classes with the maximum margin.\n",
    "* Kernel Trick: Used to map input data into a higher-dimensional space to handle non-linear separations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffd7f2-4005-4e3b-99a0-85af040ff247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train SVM model with linear kernel\n",
    "svm_model = SVC(kernel='linear') # different kernels are available (linear, polynomial, radial)\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = svm_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3847801c-c2a8-4ca8-988f-c33780320c5d",
   "metadata": {},
   "source": [
    "# 6.5. K-Nearest Neighbors (KNN)\n",
    "**Key Concepts:**\n",
    "* **KNN:** A simple, instance-based learning algorithm that classifies a data point based on the majority class of its k-nearest neighbors.\n",
    "* **Distance Metrics:** The most common are Euclidean distance and Manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed527517-12ea-4952-8f8b-03ddcd6f1485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train KNN model\n",
    "# Tune the value of the number of neighbors and observe its effect on performance\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = knn_model.predict(X_test)\n",
    "print(classification_report(y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b34f37-57ce-4b77-9ae9-ed9564a6af69",
   "metadata": {},
   "source": [
    "**Homework:**\n",
    "* Implement a machine learning pipeline that includes preprocessing, model training, and evaluation on a new dataset (e.g., a Kaggle dataset).\n",
    "* Experiment with hyperparameter tuning and model selection techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
